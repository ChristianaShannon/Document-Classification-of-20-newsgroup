{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import codecs\n",
    "import pickle\n",
    "import sys\n",
    "import tarfile\n",
    "import zipfile \n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from gensim.models import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入需要的基本库\n",
    "#### 定义文件名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CACHE_NAME = \"20news-bydate.pkz\"\n",
    "TRAIN_FOLDER = \"20news-bydate-train\"\n",
    "TEST_FOLDER = \"20news-bydate-test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#下载数据集，仅需执行一次\n",
    "def data_download(url, filename):\n",
    "    \n",
    "    #os.chdir('datasets/')\n",
    "    if not os.path.exists(filename):\n",
    "        filename,_ = urlretrieve(url+filename,filename)\n",
    "    else:\n",
    "        if filename == 'text8.zip':\n",
    "            statinfo = os.stat(filename)\n",
    "            print(\"{} has already existed,file size are {}\".format(filename, statinfo.st_size))\n",
    "        elif filename == '20news-bydate.tar.gz':\n",
    "            statinfo = os.stat(filename)\n",
    "            print(\"{} has already existed,file size are {}\".format(filename, statinfo.st_size))\n",
    "        else:\n",
    "            raise Exception(\"Please check your origin dataset.\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20news-bydate.tar.gz has already existed,file size are 14464277\n",
      "text8.zip has already existed,file size are 31344016\n"
     ]
    }
   ],
   "source": [
    "news = data_download('http://www.qwone.com/~jason/20Newsgroups/', '20news-bydate.tar.gz')\n",
    "text8 = data_download('http://mattmahoney.net/dc/', 'text8.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 解压text8词汇语料\n",
    "#完成一次后，下次不需要再解压\n",
    "def load_txts(fpath):\n",
    "    fz = zipfile.ZipFile(fpath,'r')\n",
    "    for file in fz.namelist():\n",
    "        fz.extract(file)\n",
    "    fz.close()\n",
    "text8 = load_txts('text8.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 解压20newsgroup 数据集\n",
    "#完成一次后，下次不需要再解压\n",
    "\n",
    "\n",
    "def load_categories(fpath):\n",
    "    with tarfile.open(fpath) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "    cache = dict(train=load_files(TRAIN_FOLDER, encoding='latin1'),\n",
    "                 test=load_files(TEST_FOLDER, encoding='latin1'))\n",
    "    compressed_content = codecs.encode(pickle.dumps(cache), 'zlib_codec')\n",
    "    \n",
    "    cache_path = CACHE_NAME\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        f.write(compressed_content)         #写入pkz文件\n",
    "    #return cache\n",
    "        \n",
    "load_categories('20news-bydate.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "def read_categories(subset,random_state=42):\n",
    "     if os.path.getsize(CACHE_NAME):\n",
    "        try:\n",
    "            with open(CACHE_NAME, 'rb') as f:\n",
    "                compressed_content = f.read()\n",
    "            uncompressed_content = codecs.decode(\n",
    "                compressed_content, 'zlib_codec')\n",
    "            cache = pickle.loads(uncompressed_content)  \n",
    "        except Exception as e:\n",
    "            print(80 * '_')\n",
    "            print('Cache loading failed')\n",
    "            print(80 * '_')\n",
    "            print(e) \n",
    "        data = cache[subset]\n",
    "        data.data, data.target = shuffle(data.data, data.target,random_state=random_state)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[ 7  4  4  1 14]\n"
     ]
    }
   ],
   "source": [
    "news_train = read_categories(subset='train')  #训练集\n",
    "news_test = read_categories(subset='test')   #测试集\n",
    "print(news_train.data[0])\n",
    "print(news_train.target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Text8 语料训练 word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用gensim训练词向量模型\n",
    "dim = 128\n",
    "\n",
    "text =  word2vec.Text8Corpus('text8')   #train on the pre-build text8 corpus\n",
    "w2v_model = Word2Vec(text, size = dim, min_count=1,iter = 10) #get the 128 dimensions word vector，使用CBOW模式\n",
    "w2v_model.save('word2vec_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对word2vec词向量模型进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a8610ac99385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#检测词向量模型：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw2v_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deep'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "#检测词向量模型：\n",
    "w2v_model=Word2Vec.load('word2vec_model')\n",
    "\n",
    "w2v_model.wv['deep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is to his as she is to her\n",
      "big is to bigger as heavy is to heavier\n",
      "shanghai is to china as paris is to france\n",
      "cereal\n"
     ]
    }
   ],
   "source": [
    "word_example =['he his she','big bigger heavy','shanghai china paris']\n",
    "for example in word_example:\n",
    "    a,b,x = example.split()\n",
    "    pred = w2v_model.wv.most_similar(positive=[x,b], negative=[a])[0][0]\n",
    "    print('{} is to {} as {} is to {}'.format(a,b,x,pred))\n",
    "print(w2v_model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建基于训练的 text8 模型 的字典序列word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获得基于词频v.index 的词频统计，eg. model_index_word['you'] = 206 model_index_word['beauty'] = 3714\n",
    "def index_word(model):\n",
    "    temp = {}\n",
    "    for k,v in model:\n",
    "        temp[k] = v.index\n",
    "    return temp\n",
    "model_index_word = index_word(w2v_model.wv.vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获得基于model_index_word 的word_index 转换，eg. model_index_word[123] = 'you'   model_index_word[456] = 'beauty'\n",
    "def word_index(vocab_dict):\n",
    "    temp = {}\n",
    "    for word,i in enumerate(vocab_dict):\n",
    "        temp[word] = i\n",
    "    return temp\n",
    "word_index = word_index(model_index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗数据\n",
    "#### 并获得20news 基于text8 词向量模型 的word to index 字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 7532\n",
      "['from', 'lerxst', 'wam', 'umd', 'edu', 'where', 's', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac3', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', '15', 'i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', 'it', 'was', 'a', '2', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', '60s', 'early', '70s', 'it', 'was', 'called', 'a', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'i', 'know', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'e', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "#去除各种标点符号和标签，获得清洗过的数据\n",
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \n",
    "\n",
    "    #new_text =[]\n",
    "    #snowball = nltk.stem.SnowballStemmer('english')  \n",
    "    #text = [snowball.stem(word) for word in text.split()]\n",
    "    #text = ' '.join(text)\n",
    "\n",
    "    return text.strip()\n",
    "train_data = list(map(preprocessor,news_train.data))\n",
    "test_data = list(map(preprocessor,news_test.data))\n",
    "\n",
    "print(len(train_data),len(test_data))\n",
    "print(train_data[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX_raw = []\n",
    "testX_raw = []\n",
    "\n",
    "for sentence in train_data: \n",
    "    tmp = []\n",
    "    for word in sentence.split():\n",
    "        if word in model_index_word:\n",
    "            tmp.append(model_index_word[word])\n",
    "        else:\n",
    "            tmp.append(len(model_index_word))\n",
    "    trainX_raw.append(tmp)\n",
    "    \n",
    "for sentence in test_data:\n",
    "    tmp = []\n",
    "    for word in sentence.split():\n",
    "        if word in model_index_word:\n",
    "            tmp.append(model_index_word[word])\n",
    "        else:\n",
    "            tmp.append(len(model_index_word))\n",
    "    testX_raw.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理数据，使用Keras API 获得数据的word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D,Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# 文本序列长度\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "# 单词最大数量\n",
    "MAX_NUM_WORDS = 12000\n",
    "# 词向量长度\n",
    "EMBEDDING_DIM = w2v_model.wv.syn0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(trainX_raw, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(testX_raw, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# Converting labels to binary vectors\n",
    "y_train = to_categorical(np.asarray(news_train.target))\n",
    "y_test = to_categorical(np.asarray(news_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 构建embedding矩阵，对存在于字表中的单词，使用已经训练好的w2v_model模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros([(len(word_index)+1), EMBEDDING_DIM])\n",
    "embedding_matrix[:-1] = w2v_model.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. embedding_layer 层，该层设置为trainable=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 神经网络搭建，  Embedding --> Dropout -->  LSTM --> Dense -->Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7cfa965582ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtensorBoard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./output/output/logs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msequence_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0membedded_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "#early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=25)\n",
    "tensorBoard = TensorBoard(log_dir = './output/output/logs')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Dropout(0.5)(embedded_sequences)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "#x = MaxPooling1D(5)(x)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "#x = MaxPooling1D(5)(x)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "#x = MaxPooling1D(5)(x)\n",
    "#x = GlobalMaxPooling1D()(x)\n",
    "x = LSTM(128)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(y_train.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/50\n",
      "11314/11314 [==============================] - 23s - loss: 2.8982 - acc: 0.1062 - val_loss: 2.8997 - val_acc: 0.1127\n",
      "Epoch 2/50\n",
      "11314/11314 [==============================] - 23s - loss: 2.5483 - acc: 0.1884 - val_loss: 2.5052 - val_acc: 0.1686\n",
      "Epoch 3/50\n",
      "11314/11314 [==============================] - 23s - loss: 2.3481 - acc: 0.2317 - val_loss: 3.2627 - val_acc: 0.1087\n",
      "Epoch 4/50\n",
      "11314/11314 [==============================] - 23s - loss: 2.2104 - acc: 0.2715 - val_loss: 2.1039 - val_acc: 0.2943\n",
      "Epoch 5/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.9972 - acc: 0.3249 - val_loss: 2.0213 - val_acc: 0.3198\n",
      "Epoch 6/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.8758 - acc: 0.3733 - val_loss: 2.1578 - val_acc: 0.3129\n",
      "Epoch 7/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.7507 - acc: 0.4147 - val_loss: 1.9002 - val_acc: 0.3804\n",
      "Epoch 8/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.6532 - acc: 0.4421 - val_loss: 1.8449 - val_acc: 0.4087\n",
      "Epoch 9/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.5605 - acc: 0.4715 - val_loss: 1.6477 - val_acc: 0.4505\n",
      "Epoch 10/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.4598 - acc: 0.4996 - val_loss: 1.7138 - val_acc: 0.4640\n",
      "Epoch 11/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.4003 - acc: 0.5219 - val_loss: 1.5177 - val_acc: 0.4916\n",
      "Epoch 12/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.3393 - acc: 0.5365 - val_loss: 1.4509 - val_acc: 0.5138\n",
      "Epoch 13/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.2814 - acc: 0.5602 - val_loss: 1.5381 - val_acc: 0.4870\n",
      "Epoch 14/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.2265 - acc: 0.5804 - val_loss: 1.5682 - val_acc: 0.4888\n",
      "Epoch 15/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.1829 - acc: 0.5933 - val_loss: 1.4500 - val_acc: 0.5268\n",
      "Epoch 16/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.1437 - acc: 0.6100 - val_loss: 1.3545 - val_acc: 0.5422\n",
      "Epoch 17/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.0900 - acc: 0.6205 - val_loss: 1.2936 - val_acc: 0.5725\n",
      "Epoch 18/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.0486 - acc: 0.6364 - val_loss: 1.3664 - val_acc: 0.5506\n",
      "Epoch 19/50\n",
      "11314/11314 [==============================] - 23s - loss: 1.0174 - acc: 0.6516 - val_loss: 1.2721 - val_acc: 0.5862\n",
      "Epoch 20/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.9900 - acc: 0.6616 - val_loss: 1.3186 - val_acc: 0.5807\n",
      "Epoch 21/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.9451 - acc: 0.6771 - val_loss: 1.2028 - val_acc: 0.6034\n",
      "Epoch 22/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.9225 - acc: 0.6831 - val_loss: 1.2489 - val_acc: 0.5993\n",
      "Epoch 23/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.9032 - acc: 0.6914 - val_loss: 1.2102 - val_acc: 0.6102\n",
      "Epoch 24/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.8725 - acc: 0.7003 - val_loss: 1.1751 - val_acc: 0.6202\n",
      "Epoch 25/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.8367 - acc: 0.7127 - val_loss: 1.1497 - val_acc: 0.6433\n",
      "Epoch 26/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.8153 - acc: 0.7241 - val_loss: 1.2781 - val_acc: 0.6093\n",
      "Epoch 27/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.7988 - acc: 0.7272 - val_loss: 1.1867 - val_acc: 0.6261\n",
      "Epoch 28/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.7856 - acc: 0.7358 - val_loss: 1.2599 - val_acc: 0.6138\n",
      "Epoch 29/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.7567 - acc: 0.7396 - val_loss: 1.1039 - val_acc: 0.6575\n",
      "Epoch 30/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.7250 - acc: 0.7533 - val_loss: 1.1733 - val_acc: 0.6430\n",
      "Epoch 31/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.7031 - acc: 0.7590 - val_loss: 1.2195 - val_acc: 0.6301\n",
      "Epoch 32/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.6985 - acc: 0.7607 - val_loss: 1.1197 - val_acc: 0.6644\n",
      "Epoch 33/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.6750 - acc: 0.7696 - val_loss: 1.1608 - val_acc: 0.6515\n",
      "Epoch 34/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.6596 - acc: 0.7734 - val_loss: 1.1190 - val_acc: 0.6661\n",
      "Epoch 35/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.6397 - acc: 0.7820 - val_loss: 1.1173 - val_acc: 0.6721\n",
      "Epoch 36/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.6410 - acc: 0.7849 - val_loss: 1.1562 - val_acc: 0.6646\n",
      "Epoch 37/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.6167 - acc: 0.7873 - val_loss: 1.1284 - val_acc: 0.6683\n",
      "Epoch 38/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.6034 - acc: 0.7965 - val_loss: 1.1809 - val_acc: 0.6556\n",
      "Epoch 39/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5881 - acc: 0.7987 - val_loss: 1.0992 - val_acc: 0.6792\n",
      "Epoch 40/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5853 - acc: 0.8026 - val_loss: 1.1449 - val_acc: 0.6673\n",
      "Epoch 41/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5719 - acc: 0.8040 - val_loss: 1.0928 - val_acc: 0.6839\n",
      "Epoch 42/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5444 - acc: 0.8150 - val_loss: 1.1128 - val_acc: 0.6803\n",
      "Epoch 43/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5475 - acc: 0.8153 - val_loss: 1.1044 - val_acc: 0.6904\n",
      "Epoch 44/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5325 - acc: 0.8154 - val_loss: 1.1259 - val_acc: 0.6807\n",
      "Epoch 45/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5169 - acc: 0.8233 - val_loss: 1.1393 - val_acc: 0.6815\n",
      "Epoch 46/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5142 - acc: 0.8255 - val_loss: 1.1355 - val_acc: 0.6798\n",
      "Epoch 47/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5161 - acc: 0.8227 - val_loss: 1.1093 - val_acc: 0.6913\n",
      "Epoch 48/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.5007 - acc: 0.8271 - val_loss: 1.0427 - val_acc: 0.7065\n",
      "Epoch 49/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.4903 - acc: 0.8345 - val_loss: 1.1476 - val_acc: 0.6891\n",
      "Epoch 50/50\n",
      "11314/11314 [==============================] - 23s - loss: 0.4694 - acc: 0.8409 - val_loss: 1.0923 - val_acc: 0.7033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f923432f6d8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=50,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[tensorBoard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./output/20news_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532/7532 [==============================] - 23s    \n",
      "Test score is: 1.0922839744763366\n",
      "Test accuray is: 0.7032660648218825\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test, y_test)\n",
    "print('Test score is: {}'.format(result[0]))\n",
    "print('Test accuray is: {}'.format(result[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
