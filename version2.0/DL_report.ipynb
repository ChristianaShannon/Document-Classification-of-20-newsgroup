{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import sys\n",
    "import zipfile \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载额外训练语料text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#下载数据集，仅需执行一次\n",
    "def data_download(url, filename):\n",
    "    \n",
    "    #os.chdir('datasets/')\n",
    "    if not os.path.exists(filename):\n",
    "        filename,_ = urlretrieve(url+filename,filename)\n",
    "    else:\n",
    "        if filename == 'text8.zip':\n",
    "            statinfo = os.stat(filename)\n",
    "            print(\"{} has already existed,file size are {}\".format(filename, statinfo.st_size))\n",
    "        else:\n",
    "            raise Exception(\"Please check your origin dataset.\")\n",
    "    return filename\n",
    "text8 = data_download('http://mattmahoney.net/dc/', 'text8.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 解压text8词汇语料\n",
    "#完成一次后，下次不需要再解压\n",
    "def load_txts(fpath):\n",
    "    fz = zipfile.ZipFile(fpath,'r')\n",
    "    for file in fz.namelist():\n",
    "        fz.extract(file)\n",
    "    fz.close()\n",
    "text8 = load_txts('text8.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Text8 语料训练 word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用gensim训练词向量模型\n",
    "dim = 128\n",
    "text =  word2vec.Text8Corpus('text8')   #train on the pre-build text8 corpus\n",
    "w2v_model = Word2Vec(text, size = dim, min_count=1,iter = 10) #get the 128 dimensions word vector，使用CBOW模式\n",
    "w2v_model.save('word2vec_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对word2vec词向量模型进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#检测词向量模型：\n",
    "w2v_model=Word2Vec.load('word2vec_model')\n",
    "w2v_model.wv.save_word2vec_format('text8Vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03512759 -0.00424395 -0.2512604  -0.88085592  0.03835738  0.54644412\n",
      "  1.61797595 -0.0516028  -0.43942273  1.07317543 -0.06711469 -1.15136158\n",
      " -1.24386573 -0.35904369  0.20977189 -0.17898694  0.21471363 -1.29866695\n",
      "  0.35908294  0.43134731  0.76700592 -3.45465779 -0.53539431 -2.40553665\n",
      " -0.4698481  -2.31175327 -1.20812631 -0.45274791  1.35530138  3.15746403\n",
      "  0.39651179 -0.2127454  -0.80603361 -0.60038775 -0.11123826  0.81275958\n",
      "  0.22245947 -0.5842241   0.88349861  0.38352695 -0.68967813  1.69892669\n",
      " -1.95197868  0.08851481  1.53950238 -1.62948072  0.91869867  2.31755233\n",
      "  1.58886206  1.05564499 -1.67616487  0.31222013  1.23614001 -1.41685343\n",
      " -0.33679372 -1.86449111 -0.09396074  2.12119722  1.88810432  2.69032288\n",
      "  1.26136065  0.44184107  0.58816493  1.27321422 -0.01046323  2.08304119\n",
      "  0.02146814 -1.14498115 -0.17239916 -0.33008128  1.2449472   0.98431587\n",
      "  1.2882812   0.7654416   0.43462437  0.97446895  0.57563263  0.35506377\n",
      "  0.50279886 -0.26539755  0.19651802  2.14171195  0.1833673  -0.43710238\n",
      "  1.00810421  1.42751169 -1.32431126 -0.25489241 -1.1224978  -2.04559159\n",
      "  0.85871571 -0.15902008  2.54718137  0.94597399 -0.59065086 -2.14306474\n",
      "  1.49120653  0.45164913 -0.8916707   0.19444756  2.03299499 -0.58930498\n",
      "  1.38029528  0.42467132 -0.27464941  0.17461854  1.40929329 -0.0952875\n",
      "  0.29244867  1.20284891  1.84693468  0.33149743 -0.16015883 -2.86621928\n",
      " -0.49516112 -0.83294094 -1.10685158 -0.91375381  1.09229612 -0.31711417\n",
      "  2.46123052  0.45814109 -0.94716102 -0.15093631 -0.79747444  0.82651275\n",
      " -2.10919237  0.6896224 ]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv['deep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is to his as she is to her\n",
      "big is to bigger as heavy is to lighter\n",
      "shanghai is to china as paris is to france\n",
      "cereal\n"
     ]
    }
   ],
   "source": [
    "word_example =['he his she','big bigger heavy','shanghai china paris']\n",
    "for example in word_example:\n",
    "    a,b,x = example.split()\n",
    "    pred = w2v_model.wv.most_similar(positive=[x,b], negative=[a])[0][0]\n",
    "    print('{} is to {} as {} is to {}'.format(a,b,x,pred))\n",
    "print(w2v_model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 253855 word vectors.\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('./', 'text8Vec.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理数据，使用Keras API 获得数据的word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D,Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# 文本序列长度\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "# 单词最大数量\n",
    "MAX_NUM_WORDS = 12000\n",
    "# 词向量长度\n",
    "EMBEDDING_DIM = w2v_model.wv.syn0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "targets = newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179209 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(newsgroups.data)\n",
    "sequences = tokenizer.texts_to_sequences(newsgroups.data)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 构建embedding矩阵，对存在于字表中的单词，使用已经训练好的w2v_model模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. embedding_layer 层，该层设置为trainable=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 1 神经网络搭建，  Embedding --> Dropout -->  LSTM --> Dense -->Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,TensorBoard\n",
    "\n",
    "# 通过early_stop在精度不再提高的时候停止\n",
    "#early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=0)\n",
    "tensorBoard = TensorBoard(log_dir = './output/logs')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Dropout(0.2)(embedded_sequences)\n",
    "x = LSTM(256,recurrent_dropout=0.2)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15077 samples, validate on 3769 samples\n",
      "Epoch 1/20\n",
      "15077/15077 [==============================] - 111s - loss: 2.6841 - acc: 0.1571 - val_loss: 2.3911 - val_acc: 0.2247\n",
      "Epoch 2/20\n",
      "15077/15077 [==============================] - 111s - loss: 2.1702 - acc: 0.2936 - val_loss: 1.9043 - val_acc: 0.3733\n",
      "Epoch 3/20\n",
      "15077/15077 [==============================] - 110s - loss: 1.7601 - acc: 0.4119 - val_loss: 1.5117 - val_acc: 0.4853\n",
      "Epoch 4/20\n",
      "15077/15077 [==============================] - 109s - loss: 1.3878 - acc: 0.5279 - val_loss: 1.2162 - val_acc: 0.5813\n",
      "Epoch 5/20\n",
      "15077/15077 [==============================] - 109s - loss: 1.1528 - acc: 0.6014 - val_loss: 1.0389 - val_acc: 0.6474\n",
      "Epoch 6/20\n",
      "15077/15077 [==============================] - 109s - loss: 0.9788 - acc: 0.6725 - val_loss: 0.9393 - val_acc: 0.6800\n",
      "Epoch 7/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.8637 - acc: 0.7137 - val_loss: 0.8587 - val_acc: 0.7089\n",
      "Epoch 8/20\n",
      "15077/15077 [==============================] - 109s - loss: 0.7557 - acc: 0.7555 - val_loss: 0.7943 - val_acc: 0.7424\n",
      "Epoch 9/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.6740 - acc: 0.7864 - val_loss: 0.7207 - val_acc: 0.7700\n",
      "Epoch 10/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.6035 - acc: 0.8049 - val_loss: 0.6774 - val_acc: 0.7899\n",
      "Epoch 11/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.5416 - acc: 0.8295 - val_loss: 0.6542 - val_acc: 0.7930\n",
      "Epoch 12/20\n",
      "15077/15077 [==============================] - 109s - loss: 0.5011 - acc: 0.8433 - val_loss: 0.6279 - val_acc: 0.8060\n",
      "Epoch 13/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.4612 - acc: 0.8544 - val_loss: 0.6051 - val_acc: 0.8084\n",
      "Epoch 14/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.4227 - acc: 0.8683 - val_loss: 0.5816 - val_acc: 0.8204\n",
      "Epoch 15/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.3897 - acc: 0.8784 - val_loss: 0.5728 - val_acc: 0.8156\n",
      "Epoch 16/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.3540 - acc: 0.8896 - val_loss: 0.5663 - val_acc: 0.8267\n",
      "Epoch 17/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.3242 - acc: 0.9016 - val_loss: 0.5483 - val_acc: 0.8299\n",
      "Epoch 18/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.2955 - acc: 0.9103 - val_loss: 0.5391 - val_acc: 0.8368\n",
      "Epoch 19/20\n",
      "15077/15077 [==============================] - 108s - loss: 0.2739 - acc: 0.9144 - val_loss: 0.5423 - val_acc: 0.8395\n",
      "Epoch 20/20\n",
      "15077/15077 [==============================] - 109s - loss: 0.3964 - acc: 0.8758 - val_loss: 0.5723 - val_acc: 0.8214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49f812bd68>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[tensorBoard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./20news_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3769/3769 [==============================] - 42s    \n",
      "Test score is: 0.5723360699015101\n",
      "Test accuray is: 0.8214380471957524\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_val, y_val)\n",
    "print('Test score is: {}'.format(result[0]))\n",
    "print('Test accuray is: {}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 2 神经网络搭建，  Embedding --> Dropout -->  GRU --> Dense -->Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "tensorBoard = TensorBoard(log_dir = './output/logs')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Dropout(0.2)(embedded_sequences)\n",
    "x = GRU(256,recurrent_dropout=0.2)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15077 samples, validate on 3769 samples\n",
      "Epoch 1/20\n",
      "15077/15077 [==============================] - 86s - loss: 2.8370 - acc: 0.1330 - val_loss: 2.6030 - val_acc: 0.1860\n",
      "Epoch 2/20\n",
      "15077/15077 [==============================] - 86s - loss: 2.4077 - acc: 0.2297 - val_loss: 2.0965 - val_acc: 0.3168\n",
      "Epoch 3/20\n",
      "15077/15077 [==============================] - 86s - loss: 1.8125 - acc: 0.4078 - val_loss: 1.4177 - val_acc: 0.5428\n",
      "Epoch 4/20\n",
      "15077/15077 [==============================] - 86s - loss: 1.2610 - acc: 0.5946 - val_loss: 1.0457 - val_acc: 0.6593\n",
      "Epoch 5/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.9837 - acc: 0.6830 - val_loss: 0.8751 - val_acc: 0.7225\n",
      "Epoch 6/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.8194 - acc: 0.7425 - val_loss: 0.7811 - val_acc: 0.7562\n",
      "Epoch 7/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.7132 - acc: 0.7767 - val_loss: 0.7114 - val_acc: 0.7750\n",
      "Epoch 8/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.6297 - acc: 0.8045 - val_loss: 0.6741 - val_acc: 0.7854\n",
      "Epoch 9/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.5664 - acc: 0.8263 - val_loss: 0.6319 - val_acc: 0.7928\n",
      "Epoch 10/20\n",
      "15077/15077 [==============================] - 85s - loss: 0.5056 - acc: 0.8465 - val_loss: 0.5980 - val_acc: 0.8135\n",
      "Epoch 11/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.4560 - acc: 0.8589 - val_loss: 0.5750 - val_acc: 0.8206\n",
      "Epoch 12/20\n",
      "15077/15077 [==============================] - 85s - loss: 0.4222 - acc: 0.8723 - val_loss: 0.5508 - val_acc: 0.8305\n",
      "Epoch 13/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.3843 - acc: 0.8804 - val_loss: 0.5284 - val_acc: 0.8371\n",
      "Epoch 14/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.3514 - acc: 0.8933 - val_loss: 0.5329 - val_acc: 0.8371\n",
      "Epoch 15/20\n",
      "15077/15077 [==============================] - 85s - loss: 0.3309 - acc: 0.8973 - val_loss: 0.5125 - val_acc: 0.8453\n",
      "Epoch 16/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.2939 - acc: 0.9099 - val_loss: 0.5081 - val_acc: 0.8496\n",
      "Epoch 17/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.2752 - acc: 0.9175 - val_loss: 0.4978 - val_acc: 0.8549\n",
      "Epoch 18/20\n",
      "15077/15077 [==============================] - 85s - loss: 0.2518 - acc: 0.9250 - val_loss: 0.4947 - val_acc: 0.8522\n",
      "Epoch 19/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.2403 - acc: 0.9282 - val_loss: 0.5052 - val_acc: 0.8527\n",
      "Epoch 20/20\n",
      "15077/15077 [==============================] - 86s - loss: 0.2197 - acc: 0.9326 - val_loss: 0.4867 - val_acc: 0.8538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49bf96ffd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[tensorBoard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./20news_GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3769/3769 [==============================] - 32s    \n",
      "Test score is: 0.48670502125220577\n",
      "Test accuray is: 0.8538073759301646\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_val, y_val)\n",
    "print('Test score is: {}'.format(result[0]))\n",
    "print('Test accuray is: {}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 3 神经网络搭建，  Embedding --> Conv1D --> MaxPooling --> Dropout -->  LSTM --> Dense -->Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorBoard = TensorBoard(log_dir = './output/logs')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(32, 3, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(256,recurrent_dropout=0.2)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15077 samples, validate on 3769 samples\n",
      "Epoch 1/20\n",
      "15077/15077 [==============================] - 48s - loss: 2.6936 - acc: 0.1417 - val_loss: 2.4218 - val_acc: 0.1693\n",
      "Epoch 2/20\n",
      "15077/15077 [==============================] - 37s - loss: 2.2704 - acc: 0.2216 - val_loss: 2.1077 - val_acc: 0.2653\n",
      "Epoch 3/20\n",
      "15077/15077 [==============================] - 37s - loss: 2.0467 - acc: 0.2833 - val_loss: 1.9100 - val_acc: 0.3447\n",
      "Epoch 4/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.8612 - acc: 0.3481 - val_loss: 1.7095 - val_acc: 0.4025\n",
      "Epoch 5/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.6784 - acc: 0.4188 - val_loss: 1.6145 - val_acc: 0.4452\n",
      "Epoch 6/20\n",
      "15077/15077 [==============================] - 36s - loss: 1.5289 - acc: 0.4644 - val_loss: 1.4298 - val_acc: 0.5041\n",
      "Epoch 7/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.3743 - acc: 0.5163 - val_loss: 1.3163 - val_acc: 0.5304\n",
      "Epoch 8/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.2731 - acc: 0.5514 - val_loss: 1.2071 - val_acc: 0.5726\n",
      "Epoch 9/20\n",
      "15077/15077 [==============================] - 36s - loss: 1.1872 - acc: 0.5825 - val_loss: 1.1375 - val_acc: 0.6020\n",
      "Epoch 10/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.1038 - acc: 0.6109 - val_loss: 1.0680 - val_acc: 0.6323\n",
      "Epoch 11/20\n",
      "15077/15077 [==============================] - 36s - loss: 1.0474 - acc: 0.6333 - val_loss: 1.0549 - val_acc: 0.6354\n",
      "Epoch 12/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.9893 - acc: 0.6544 - val_loss: 0.9785 - val_acc: 0.6652\n",
      "Epoch 13/20\n",
      "15077/15077 [==============================] - 36s - loss: 0.9379 - acc: 0.6720 - val_loss: 0.9603 - val_acc: 0.6726\n",
      "Epoch 14/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.8870 - acc: 0.6936 - val_loss: 0.9292 - val_acc: 0.6806\n",
      "Epoch 15/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.8483 - acc: 0.7013 - val_loss: 0.8978 - val_acc: 0.6920\n",
      "Epoch 16/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.8145 - acc: 0.7154 - val_loss: 0.8903 - val_acc: 0.7005\n",
      "Epoch 17/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.7795 - acc: 0.7281 - val_loss: 0.8890 - val_acc: 0.6917\n",
      "Epoch 18/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.7544 - acc: 0.7427 - val_loss: 0.8251 - val_acc: 0.7188\n",
      "Epoch 19/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.7078 - acc: 0.7567 - val_loss: 0.8122 - val_acc: 0.7307\n",
      "Epoch 20/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.6909 - acc: 0.7662 - val_loss: 0.8217 - val_acc: 0.7280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49be5a09b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[tensorBoard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./20news_CNN_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3769/3769 [==============================] - 14s    \n",
      "Test score is: 0.8217184117884748\n",
      "Test accuray is: 0.7280445738413126\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_val, y_val)\n",
    "print('Test score is: {}'.format(result[0]))\n",
    "print('Test accuray is: {}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 4 神经网络搭建，  Embedding --> Conv1D --> MaxPooling --> Dropout -->  GRU --> Dense -->Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorBoard = TensorBoard(log_dir = './output/logs')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(32, 3, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = GRU(256,recurrent_dropout=0.2)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15077 samples, validate on 3769 samples\n",
      "Epoch 1/20\n",
      "15077/15077 [==============================] - 37s - loss: 2.6550 - acc: 0.1390 - val_loss: 2.3212 - val_acc: 0.1897\n",
      "Epoch 2/20\n",
      "15077/15077 [==============================] - 37s - loss: 2.2108 - acc: 0.2381 - val_loss: 2.0497 - val_acc: 0.2903\n",
      "Epoch 3/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.9865 - acc: 0.3150 - val_loss: 1.8669 - val_acc: 0.3786\n",
      "Epoch 4/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.7842 - acc: 0.3858 - val_loss: 1.6869 - val_acc: 0.4187\n",
      "Epoch 5/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.6069 - acc: 0.4376 - val_loss: 1.4820 - val_acc: 0.4747\n",
      "Epoch 6/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.4610 - acc: 0.4898 - val_loss: 1.3840 - val_acc: 0.4930\n",
      "Epoch 7/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.3199 - acc: 0.5309 - val_loss: 1.2655 - val_acc: 0.5527\n",
      "Epoch 8/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.2295 - acc: 0.5620 - val_loss: 1.1701 - val_acc: 0.5882\n",
      "Epoch 9/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.1650 - acc: 0.5834 - val_loss: 1.1559 - val_acc: 0.5949\n",
      "Epoch 10/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.0908 - acc: 0.6143 - val_loss: 1.0697 - val_acc: 0.6291\n",
      "Epoch 11/20\n",
      "15077/15077 [==============================] - 37s - loss: 1.0349 - acc: 0.6377 - val_loss: 1.0484 - val_acc: 0.6288\n",
      "Epoch 12/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.9782 - acc: 0.6570 - val_loss: 1.0127 - val_acc: 0.6495\n",
      "Epoch 13/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.9414 - acc: 0.6691 - val_loss: 0.9479 - val_acc: 0.6721\n",
      "Epoch 14/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.8919 - acc: 0.6872 - val_loss: 0.9366 - val_acc: 0.6744\n",
      "Epoch 15/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.8662 - acc: 0.7005 - val_loss: 0.9065 - val_acc: 0.6885\n",
      "Epoch 16/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.8099 - acc: 0.7180 - val_loss: 0.8622 - val_acc: 0.7047\n",
      "Epoch 17/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.7765 - acc: 0.7312 - val_loss: 0.8627 - val_acc: 0.7023\n",
      "Epoch 18/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.7526 - acc: 0.7401 - val_loss: 0.8151 - val_acc: 0.7233\n",
      "Epoch 19/20\n",
      "15077/15077 [==============================] - 36s - loss: 0.7186 - acc: 0.7576 - val_loss: 0.8045 - val_acc: 0.7304\n",
      "Epoch 20/20\n",
      "15077/15077 [==============================] - 37s - loss: 0.6821 - acc: 0.7683 - val_loss: 0.7878 - val_acc: 0.7341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49b277da90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[tensorBoard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./20news_CNN_GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3769/3769 [==============================] - 14s    \n",
      "Test score is: 0.7877520742989498\n",
      "Test accuray is: 0.7341469882748494\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_val, y_val)\n",
    "print('Test score is: {}'.format(result[0]))\n",
    "print('Test accuray is: {}'.format(result[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
